{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "[0 0 2 ... 3 6 2]\n",
      "(35546, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import math\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector,Input\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Add,Concatenate,BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#參數\n",
    "MAX_SENT_LENGTH = 8\n",
    "MAX_SENTS = 64\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "\n",
    "## label data\n",
    "\n",
    "print (keras.__version__)\n",
    "\n",
    "column_names = ['type','title','text']\n",
    "df = pd.read_csv('./all_after_mapping.tsv',sep='\\t',names=column_names)\n",
    "\n",
    "labels = df['type'].values\n",
    "labels = np.array(labels)\n",
    "print(labels)\n",
    "labels = np_utils.to_categorical(labels)\n",
    "print(labels.shape)\n",
    "tokenlizeword = np.load('./tokenlizeword0221.npy',allow_pickle=True)\n",
    "u_tokenlizeword = np.load('./tokenlizeword0226_udn_for_mct.npy',allow_pickle=True)\n",
    "#串起來\n",
    "tokenlizeword=np.concatenate((tokenlizeword, u_tokenlizeword), axis=0)\n",
    "\n",
    "wmodel = Word2Vec(tokenlizeword, size=300, window=5, min_count=0)\n",
    "# wmodel=Word2Vec.load('./word2vec.model')\n",
    "# wmodel.most_similar('我')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46237\n",
      "embedding done\n"
     ]
    }
   ],
   "source": [
    "#x_train=文章數*最大句數*最大字數*300維\n",
    "x_train =[] #label data\n",
    "##文章數\n",
    "for k in range(tokenlizeword.shape[0]):\n",
    "    embedding_matrix=np.zeros((MAX_SENTS,MAX_SENT_LENGTH,EMBEDDING_DIM))\n",
    "    word_count=0 ##計算目前位置\n",
    "    ##句子數\n",
    "    for i in range(MAX_SENTS):\n",
    "        if word_count >= len(tokenlizeword[k]):\n",
    "            break\n",
    "        embedding_vector=np.zeros((MAX_SENT_LENGTH,EMBEDDING_DIM))\n",
    "        ##最大字數\n",
    "        for j in range(MAX_SENT_LENGTH):\n",
    "            if tokenlizeword[k][word_count] == '，' or tokenlizeword[k][word_count] == '。' or tokenlizeword[k][word_count] == '！' or tokenlizeword[k][word_count] == '？':\n",
    "                word_count=word_count+1\n",
    "                break\n",
    "            else:\n",
    "                embedding_vector[j]=wmodel[tokenlizeword[k][word_count]]\n",
    "                word_count=word_count+1\n",
    "                if word_count >= len(tokenlizeword[k]):\n",
    "                    break\n",
    "        embedding_matrix[i]=embedding_vector\n",
    "    x_train.append(embedding_matrix)\n",
    "\n",
    "print(len(x_train))\n",
    "print(\"embedding done\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 64, 8, 300)\n",
      "(5000, 64, 8, 300)\n",
      "(25546, 64, 8, 300)\n",
      "60%\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(x_train,dtype = 'float32')\n",
    "\n",
    "\n",
    "dev_x = x_train[:5000]\n",
    "dev_y = labels[:5000]\n",
    "\n",
    "test_x = x_train[5000:10000]\n",
    "test_y = labels[5000:10000]\n",
    "#到35546為止為label data\n",
    "train_x = x_train[10000:35546]\n",
    "\n",
    "train_y = labels[10000:]\n",
    "# !ls\n",
    "print(dev_x.shape)\n",
    "print(test_x.shape)\n",
    "print(train_x.shape)\n",
    "\n",
    "print(\"60%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意層\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = 50\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], 1)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "            \n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'attention_25/Variable:0' shape=(100, 1) dtype=float32> W\n",
      "tracking <tf.Variable 'attention_25/Variable_1:0' shape=(50,) dtype=float32> b\n",
      "tracking <tf.Variable 'attention_25/Variable_2:0' shape=(50, 1) dtype=float32> u\n",
      "tracking <tf.Variable 'attention_26/Variable:0' shape=(100, 1) dtype=float32> W\n",
      "tracking <tf.Variable 'attention_26/Variable_1:0' shape=(50,) dtype=float32> b\n",
      "tracking <tf.Variable 'attention_26/Variable_2:0' shape=(50, 1) dtype=float32> u\n",
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        (None, 64, 8, 300)        0         \n",
      "_________________________________________________________________\n",
      "time_distributed_39 (TimeDis (None, 64, 100)           260900    \n",
      "_________________________________________________________________\n",
      "bidirectional_27 (Bidirectio (None, 64, 200)           120600    \n",
      "_________________________________________________________________\n",
      "time_distributed_40 (TimeDis (None, 64, 100)           20100     \n",
      "_________________________________________________________________\n",
      "attention_26 (Attention)     (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 402,507\n",
      "Trainable params: 402,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model fitting - Hierachical attention network\n",
      "Train on 25546 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "25546/25546 [==============================] - 118s 5ms/step - loss: 0.4012 - acc: 0.8654 - val_loss: 0.8216 - val_acc: 0.7746\n",
      "Epoch 2/3\n",
      "25546/25546 [==============================] - 116s 5ms/step - loss: 0.2739 - acc: 0.9045 - val_loss: 0.7162 - val_acc: 0.8010\n",
      "Epoch 3/3\n",
      "25546/25546 [==============================] - 116s 5ms/step - loss: 0.2254 - acc: 0.9210 - val_loss: 0.7764 - val_acc: 0.7962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f9a643f38d0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(sentence_input)\n",
    "l_dense = TimeDistributed(Dense(100))(l_lstm)\n",
    "l_att = Attention()(l_dense)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(100))(l_lstm_sent)\n",
    "l_att_sent = Attention()(l_dense_sent)\n",
    "preds = Dense(7, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['acc'])\n",
    "filepath = \"./HAN.h5\"\n",
    " \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max', period=1)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.summary()\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(train_x, train_y, validation_data=(dev_x,dev_y),epochs=3, batch_size=32,callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'att_layer_4/W:0' shape=(200, 100) dtype=float32> W\n",
      "tracking <tf.Variable 'att_layer_4/b:0' shape=(100,) dtype=float32> b\n",
      "tracking <tf.Variable 'att_layer_4/u:0' shape=(100, 1) dtype=float32> u\n",
      "tracking <tf.Variable 'att_layer_5/W:0' shape=(7, 100) dtype=float32> W\n",
      "tracking <tf.Variable 'att_layer_5/b:0' shape=(100,) dtype=float32> b\n",
      "tracking <tf.Variable 'att_layer_5/u:0' shape=(100, 1) dtype=float32> u\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 64, 8, 300)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 64, 200)      260800      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64, 200)      0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 64, 64)       38464       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 64, 64)       51264       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 64, 64)       64064       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 21, 64)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 16, 64)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 12, 64)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 49, 64)       0           max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 49, 64)       0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 49, 200)      99000       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 49, 7)        1407        bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_5 (AttLayer)          (None, 7)            900         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 515,899\n",
      "Trainable params: 515,899\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "model fitting - Hierachical attention network\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 25546 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "25546/25546 [==============================] - 272s 11ms/step - loss: 0.3841 - acc: 0.8660 - val_loss: 0.8632 - val_acc: 0.7712\n",
      "Epoch 2/3\n",
      "25546/25546 [==============================] - 278s 11ms/step - loss: 0.2707 - acc: 0.9032 - val_loss: 0.8432 - val_acc: 0.7902\n",
      "Epoch 3/3\n",
      "25546/25546 [==============================] - 275s 11ms/step - loss: 0.2247 - acc: 0.9187 - val_loss: 0.8201 - val_acc: 0.7944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8cec6e7e48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "l_lstm = Bidirectional(GRU(300, return_sequences=True))(sentence_input)\n",
    "l_att = AttLayer(300)(l_lstm)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "# l_drop1 = Dropout(0.3)(review_encoder)\n",
    "# l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "dropout = Dropout(0.2)(review_encoder)\n",
    "filter_sizes = [3,4,5]\n",
    "convs = []\n",
    "for filter_size in filter_sizes:\n",
    "    conv = Conv1D(filters=64, kernel_size=filter_size, padding='same', activation='relu')(dropout)\n",
    "    pool = MaxPooling1D(filter_size)(conv)\n",
    "    convs.append(pool)\n",
    "        \n",
    "concatenate = Concatenate(axis=1)(convs)\n",
    "drop_out = Dropout(0.1)(concatenate)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(drop_out)\n",
    "preds = Dense(7, activation='softmax')(l_lstm_sent)\n",
    "l_att_sent = AttLayer(100)(preds)\n",
    "\n",
    "model = Model(review_input, l_att_sent)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "filepath = \"best_HAHNN.h5\"\n",
    " \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max', period=1)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "model.summary()\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(test_x, test_y, validation_data=(dev_x,dev_y),epochs=3, batch_size=4,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(test_x,test_y,model):\n",
    "    test_y = np.array(test_y,dtype ='int64')\n",
    "    total = len(test_x)\n",
    "    logits=model.predict(test_x,batch_size=50)\n",
    "    pred=np.argmax(logits, axis=1)\n",
    "    correct=0 #對的有幾個\n",
    "    label_list = ['政治','生活','國際','體育','娛樂','社會','財經']\n",
    "    total_labels =[0,0,0,0,0,0,0]\n",
    "    correct_labels = [0,0,0,0,0,0,0]\n",
    "    total_labels = np.array(total_labels)\n",
    "    correct_labels = np.array(correct_labels)\n",
    "    for i in range(len(test_x)):\n",
    "        total_labels += test_y[i]\n",
    "        if test_y[i][pred[i]]==1.0:\n",
    "            correct = correct+1\n",
    "            correct_labels[pred[i]] += 1\n",
    "    for i in range(len(total_labels)):\n",
    "            print(label_list[i],':',correct_labels[i],'/',total_labels[i], 'Acc :', (correct_labels[i]/total_labels[i]))\n",
    "    print('Total：',correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 有bug\n",
    "def test_acc_two_ans(test_x,test_y,model):\n",
    "    test_y = np.array(test_y,dtype ='int64')\n",
    "    total = len(test_x)\n",
    "    logits=model.predict(test_x,batch_size=50)\n",
    "    pred=np.argmax(logits, axis=1)\n",
    "    for i in range(len(logits)):\n",
    "        logits[i][pred[i]] = 0.0\n",
    "    pred_sec = np.argmax(logits,axis=1)\n",
    "    correct=0 #對的有幾個\n",
    "    label_list = ['政治','生活','國際','體育','娛樂','社會','財經']\n",
    "    total_labels =[0,0,0,0,0,0,0]\n",
    "    correct_labels = [0,0,0,0,0,0,0]\n",
    "    total_labels = np.array(total_labels)\n",
    "    correct_labels = np.array(correct_labels)\n",
    "    for i in range(len(test_x)):\n",
    "        total_labels += test_y[i]\n",
    "        if test_y[i][pred[i]]==1.0 or test_y[i][pred_sec[i]]==1.0:\n",
    "            correct = correct+1\n",
    "            correct_labels+= test_y[i]\n",
    "    for i in range(len(total_labels)):\n",
    "            print(label_list[i],':',correct_labels[i],'/',total_labels[i], 'Acc :', (correct_labels[i]/total_labels[i]))\n",
    "    print('Total：',correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "政治 : 914 / 1104 Acc : 0.8278985507246377\n",
      "生活 : 1072 / 1107 Acc : 0.9683830171635049\n",
      "國際 : 399 / 424 Acc : 0.9410377358490566\n",
      "體育 : 358 / 359 Acc : 0.9972144846796658\n",
      "娛樂 : 639 / 653 Acc : 0.9785604900459418\n",
      "社會 : 379 / 453 Acc : 0.8366445916114791\n",
      "財經 : 869 / 900 Acc : 0.9655555555555555\n",
      "Total： 0.926\n",
      "政治 : 793 / 1104 Acc : 0.7182971014492754\n",
      "生活 : 929 / 1107 Acc : 0.8392050587172538\n",
      "國際 : 314 / 424 Acc : 0.7405660377358491\n",
      "體育 : 350 / 359 Acc : 0.9749303621169917\n",
      "娛樂 : 611 / 653 Acc : 0.9356814701378254\n",
      "社會 : 307 / 453 Acc : 0.6777041942604857\n",
      "財經 : 800 / 900 Acc : 0.8888888888888888\n",
      "Total： 0.8208\n"
     ]
    }
   ],
   "source": [
    "test_acc_two_ans(test_x,test_y,model)\n",
    "test_acc(test_x,test_y,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logits=model.predict(test_x,batch_size=50)\n",
    "pred=np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4104\n",
      "0.8208\n"
     ]
    }
   ],
   "source": [
    "#驗證還沒加入unlabel data的表現\n",
    "correct=0 #對的有幾個\n",
    "for i in range(5000):\n",
    "    if test_y[i][pred[i]]==1.0:\n",
    "        correct = correct+1\n",
    "print(correct)\n",
    "acc=correct/5000\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##預測\n",
    "u_train = x_train[35546:]\n",
    "u_pred = model.predict(u_train,batch_size=50)\n",
    "ary =[] ##準備刪除的所有資料位置\n",
    "for i in range(u_train.shape[0]):\n",
    "    ans = u_pred[i][0]\n",
    "    for j in range(7):\n",
    "        if u_pred[i][j]>ans:\n",
    "            ans=u_pred[i][j]\n",
    "    if ans<acc: ##比模型準確率高\n",
    "        ary.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3992579e-01 2.1731112e-02 8.2282134e-04 7.1054637e-01 4.6229051e-04\n",
      " 2.6327234e-02 1.8443231e-04]\n",
      "(10691, 7)\n",
      "[8.8439238e-01 5.6384900e-04 5.2494352e-06 9.5021096e-06 1.8242728e-06\n",
      " 1.1501102e-01 1.6204893e-05]\n",
      "(6374, 7)\n"
     ]
    }
   ],
   "source": [
    "print(u_pred[3])\n",
    "print(u_pred.shape)\n",
    "u_pred=np.delete(u_pred,ary,axis=0) ##刪除所有資料不夠好的位置\n",
    "print(u_pred[3])\n",
    "print(u_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 44.9 GiB for an array with shape (39218, 64, 8, 300) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a76b29b958a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mary_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m35546\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mary_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mu_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(arr, obj, axis)\u001b[0m\n\u001b[1;32m   4415\u001b[0m         \u001b[0mkeep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4416\u001b[0m         \u001b[0mslobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4417\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 44.9 GiB for an array with shape (39218, 64, 8, 300) and data type float64"
     ]
    }
   ],
   "source": [
    "ary_label = []##刪除x_train用ㄉ\n",
    "\n",
    "for i in range(len(ary)):\n",
    "    ary_label.append(ary[i]+35546)\n",
    "x_train = np.delete(x_train,ary_label,axis=0)\n",
    "print(x_train.shape)\n",
    "u_pred = np.argmax(u_pred, axis=1)\n",
    "##轉one-hot\n",
    "u_pred = np.array(u_pred)\n",
    "u_pred = np_utils.to_categorical(u_pred)\n",
    "print(u_pred.shape)\n",
    "print(u_pred[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = np.argmax(u_pred, axis=1)\n",
    "##轉one-hot\n",
    "u_pred = np.array(u_pred)\n",
    "u_pred = np_utils.to_categorical(u_pred)\n",
    "print(u_pred.shape)\n",
    "print(u_pred[7])\n",
    "train_y=labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#串起來\n",
    "train_y=np.concatenate((train_y,u_pred),axis=0)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#再train一個model\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(sentence_input)\n",
    "dropout = Dropout(0.2)(l_lstm)\n",
    "filter_sizes = [3,4,5]\n",
    "convs = []\n",
    "for filter_size in filter_sizes:\n",
    "    conv = Conv1D(filters=64, kernel_size=filter_size, padding='same', activation='relu')(dropout)\n",
    "    pool = MaxPooling1D(filter_size)(conv)\n",
    "    convs.append(pool)\n",
    "        \n",
    "concatenate = Concatenate(axis=1)(convs)\n",
    "drop_out = Dropout(0.1)(concatenate)\n",
    "l_att = AttLayer(100)(drop_out)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "# l_drop1 = Dropout(0.3)(review_encoder)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_att_sent = AttLayer(100)(l_lstm_sent)\n",
    "preds = Dense(7, activation='softmax')(l_att_sent)\n",
    "u_model = Model(review_input, preds)\n",
    "\n",
    "u_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "filepath = \"best_HAN_with_unlabel.h5\"\n",
    " \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max', period=1)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "train_x=x_train[10000:]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "u_model.summary()\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "u_model.fit(train_x, train_y, validation_data=(dev_x,dev_y),epochs=3, batch_size=4,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "pred=u_model.predict(test_x,batch_size=50)\n",
    "pred=np.argmax(pred, axis=1)\n",
    "print(pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4176\n",
      "0.8352\n"
     ]
    }
   ],
   "source": [
    "#加入unlabel data後的表現\n",
    "correct=0 #對的有幾個\n",
    "for i in range(5000):\n",
    "    if test_y[i][pred[i]]==1.0:\n",
    "        correct = correct+1\n",
    "print(correct)\n",
    "acc=correct/5000\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
